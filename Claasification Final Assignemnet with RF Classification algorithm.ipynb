{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset=pd.read_csv(\"ckd.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'bp', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hrmo', 'pcv',\n",
       "       'wc', 'rc', 'sg_b', 'sg_c', 'sg_d', 'sg_e', 'rbc_normal', 'pc_normal',\n",
       "       'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes',\n",
       "       'appet_yes', 'pe_yes', 'ane_yes', 'classification_yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.get_dummies(dataset, drop_first=True)\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent=dataset[['age', 'bp', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hrmo', 'pcv',\n",
    "       'wc', 'rc', 'sg_b', 'sg_c', 'sg_d', 'sg_e', 'rbc_normal', 'pc_normal',\n",
    "       'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes',\n",
    "       'appet_yes', 'pe_yes', 'ane_yes']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dependent=dataset['classification_yes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(independent, dependent, test_size = 1/3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SC=StandardScaler()\n",
    "x_train=SC.fit_transform(x_train)\n",
    "x_test=SC.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1800 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 244, in fit\n",
      "    % self.min_samples_leaf\n",
      "ValueError: min_samples_leaf must be at least 1 or in (0, 0.5], got 1.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 244, in fit\n",
      "    % self.min_samples_leaf\n",
      "ValueError: min_samples_leaf must be at least 1 or in (0, 0.5], got 10.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 244, in fit\n",
      "    % self.min_samples_leaf\n",
      "ValueError: min_samples_leaf must be at least 1 or in (0, 0.5], got 100.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 467, in fit\n",
      "    for i, t in enumerate(trees)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\poorn\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.9510195  0.96272277 0.95153496 0.93993862 0.94775636 0.9477816\n",
      " 0.94775869 0.94773163 0.94799537        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.93667183 0.95133107 0.95882271 0.95506424 0.95882271 0.95510745\n",
      " 0.94421431 0.95532396 0.95148822        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92924524 0.95525415 0.9477816  0.95490938 0.94805324 0.9477816\n",
      " 0.93232178 0.95531766 0.95525647        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95169321 0.94774742 0.9624798  0.93293855 0.95162613 0.95148822\n",
      " 0.91436849 0.95512487 0.95533275        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95151212 0.95506074 0.9477816  0.95491642 0.95131365 0.95512487\n",
      " 0.94755126 0.95148342 0.95161982        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92522834 0.95533275 0.95518605 0.94389675 0.9440816  0.95512487\n",
      " 0.94413303 0.94406996 0.95161982        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95507799 0.95148342 0.95512487 0.95506506 0.95118791 0.95140962\n",
      " 0.92938133 0.95155249 0.95912656        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92596443 0.96253534 0.95882271 0.94393649 0.96266724 0.95512487\n",
      " 0.94430882 0.95161982 0.95533275        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'class_weight': ['balanced', 'balanced_subsample'],\n",
       "                         'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [0.1, 1.0, 10.0, 100.0],\n",
       "                         'min_samples_split': [2, 20, 100],\n",
       "                         'n_estimators': [10, 100, 1000]},\n",
       "             scoring='f1_weighted')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid= {'n_estimators':[10,100,1000],'criterion':['gini', 'entropy', 'log_loss'], 'min_samples_split':[2,20,100],'min_samples_leaf':[0.1,1.0,10.0,100.0],'max_features':['sqrt','log2'],'class_weight':['balanced','balanced_subsample']}\n",
    "\n",
    "grid=GridSearchCV(RandomForestClassifier(),param_grid,scoring='f1_weighted')\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "#print(grid.best_params_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.00725074, 0.04617038, 0.42740402, 0.00532942, 0.04328518,\n",
      "       0.42276516, 0.00547328, 0.04013958, 0.39673915, 0.00257816,\n",
      "       0.02071109, 0.19882774, 0.00313931, 0.02076254, 0.19845614,\n",
      "       0.00283446, 0.02084799, 0.19918294, 0.00280461, 0.02041764,\n",
      "       0.19887509, 0.00299153, 0.02104516, 0.19940515, 0.00275559,\n",
      "       0.02108097, 0.19985843, 0.00287709, 0.02184095, 0.20345607,\n",
      "       0.00317183, 0.02082448, 0.2037293 , 0.00266304, 0.02164793,\n",
      "       0.21050057, 0.00528779, 0.04469023, 0.41909428, 0.00547667,\n",
      "       0.05042391, 0.43070784, 0.00520349, 0.0422543 , 0.41425567,\n",
      "       0.00283442, 0.02071385, 0.20415425, 0.00321288, 0.02127995,\n",
      "       0.2137392 , 0.00286932, 0.02143664, 0.20826907, 0.00287328,\n",
      "       0.02152534, 0.20940552, 0.00310793, 0.02191586, 0.20780058,\n",
      "       0.00302677, 0.02192507, 0.21064949, 0.00288606, 0.02169027,\n",
      "       0.20909753, 0.00270495, 0.02243891, 0.21005111, 0.00306106,\n",
      "       0.02197528, 0.21082048, 0.00536046, 0.04390087, 0.44114218,\n",
      "       0.0056468 , 0.04546013, 0.4427588 , 0.00517159, 0.04162006,\n",
      "       0.41274166, 0.00325012, 0.02169662, 0.21020155, 0.00283694,\n",
      "       0.022611  , 0.21981759, 0.00308557, 0.02185307, 0.21462398,\n",
      "       0.00329866, 0.02315044, 0.2229908 , 0.00306396, 0.02298698,\n",
      "       0.22225118, 0.00306687, 0.02284203, 0.22620764, 0.00314937,\n",
      "       0.02311001, 0.22281237, 0.00308495, 0.0218307 , 0.21372733,\n",
      "       0.00326977, 0.02202878, 0.22239928, 0.00541487, 0.04621277,\n",
      "       0.44868965, 0.00533152, 0.04645486, 0.44322352, 0.00533223,\n",
      "       0.04215598, 0.4194344 , 0.00307846, 0.02174411, 0.21352291,\n",
      "       0.00306172, 0.02300735, 0.21294336, 0.00323205, 0.02228608,\n",
      "       0.21357026, 0.00306897, 0.0221839 , 0.21065907, 0.00271568,\n",
      "       0.02167492, 0.21028996, 0.00318737, 0.02177372, 0.21005311,\n",
      "       0.00272751, 0.02167592, 0.20963101, 0.00347753, 0.02195325,\n",
      "       0.20987587, 0.00273156, 0.02268171, 0.21046443, 0.0028573 ,\n",
      "       0.02152104, 0.21091194, 0.00307961, 0.02167578, 0.21206923,\n",
      "       0.00327392, 0.02226458, 0.21356277, 0.00322504, 0.02186394,\n",
      "       0.23589692, 0.0029026 , 0.02353005, 0.22332325, 0.00325494,\n",
      "       0.02352328, 0.23695998, 0.0033854 , 0.02315555, 0.21653686,\n",
      "       0.00308785, 0.02134786, 0.21066542, 0.00301275, 0.02172241,\n",
      "       0.21151123, 0.00349126, 0.02202473, 0.21270866, 0.00284052,\n",
      "       0.02222652, 0.21231833, 0.00305767, 0.02132816, 0.21380372,\n",
      "       0.00334425, 0.02178483, 0.21278048, 0.0028336 , 0.02259445,\n",
      "       0.21419053, 0.00335875, 0.02218451, 0.22327371, 0.00315652,\n",
      "       0.02325435, 0.22259965, 0.00305433, 0.02252774, 0.22225857,\n",
      "       0.00287814, 0.02347512, 0.22171617, 0.0033164 , 0.02576814,\n",
      "       0.21341009, 0.0028635 , 0.02255483, 0.2115531 , 0.00328097,\n",
      "       0.02164388, 0.21215944, 0.00307059, 0.02167788, 0.2102345 ,\n",
      "       0.00310702, 0.02234068, 0.21075244, 0.00307345, 0.02266364,\n",
      "       0.21116509, 0.00717001, 0.06109476, 0.60350637, 0.00619717,\n",
      "       0.06082621, 0.60368166, 0.00674915, 0.05901833, 0.58294687,\n",
      "       0.00270133, 0.02160316, 0.21087108, 0.00299978, 0.02241993,\n",
      "       0.21421528, 0.00334201, 0.02212815, 0.22472486, 0.00351343,\n",
      "       0.02365985, 0.21336479, 0.00329471, 0.02219377, 0.21217599,\n",
      "       0.00287614, 0.02178416, 0.21536584, 0.00286722, 0.02248778,\n",
      "       0.21210356, 0.00288873, 0.02279611, 0.2122694 , 0.0032321 ,\n",
      "       0.02187881, 0.21258159, 0.00637641, 0.06049457, 0.60115304,\n",
      "       0.00647116, 0.05945406, 0.60630827, 0.00678425, 0.05853524,\n",
      "       0.57720685, 0.003162  , 0.02262044, 0.21204343, 0.0030786 ,\n",
      "       0.02142258, 0.27055583, 0.00442762, 0.02565274, 0.24070582,\n",
      "       0.00280652, 0.02194633, 0.2217844 , 0.00310116, 0.02272854,\n",
      "       0.26497512, 0.00502172, 0.03986788, 0.2584681 , 0.00430918,\n",
      "       0.02865477, 0.23377194, 0.00328665, 0.02241635, 0.23259573,\n",
      "       0.00306635, 0.02382674, 0.24573798, 0.00740814, 0.10175867,\n",
      "       0.82667789, 0.00784478, 0.06597791, 0.62193899, 0.00657597,\n",
      "       0.06002445, 0.62692518, 0.00309801, 0.02401843, 0.22734799,\n",
      "       0.00331759, 0.02376399, 0.22671137, 0.00290813, 0.02335281,\n",
      "       0.226262  , 0.00312643, 0.02334533, 0.22546678, 0.00288701,\n",
      "       0.02382755, 0.22494049, 0.00333843, 0.02298999, 0.21026649,\n",
      "       0.0032907 , 0.02186685, 0.21058083, 0.00284595, 0.02214494,\n",
      "       0.21117506, 0.00305953, 0.02189612, 0.21151896, 0.00637841,\n",
      "       0.06086707, 0.68529387, 0.00845337, 0.0876893 , 0.65453782,\n",
      "       0.00720844, 0.07098565, 0.67557302, 0.00301738, 0.02469583,\n",
      "       0.24843316, 0.00339952, 0.02552876, 0.22839298, 0.00291409,\n",
      "       0.02290792, 0.21660738, 0.00307798, 0.02204919, 0.2134747 ,\n",
      "       0.00316749, 0.02328386, 0.2147254 , 0.00286846, 0.0234767 ,\n",
      "       0.21658831, 0.00334129, 0.02216845, 0.21717796, 0.00295253,\n",
      "       0.02253919, 0.22171011, 0.00361967, 0.02311888, 0.21534047,\n",
      "       0.00286922, 0.02294865, 0.22009158, 0.00304489, 0.02202358,\n",
      "       0.21922231, 0.00379558, 0.02370205, 0.22338581, 0.00311294,\n",
      "       0.02264481, 0.21875634, 0.00335736, 0.02284045, 0.2159235 ,\n",
      "       0.00307798, 0.02289243, 0.21728168, 0.00285211, 0.02209888,\n",
      "       0.21426225, 0.00303364, 0.02252493, 0.21565309, 0.00331664,\n",
      "       0.02239594, 0.21375756, 0.00306602, 0.02323189, 0.21605406,\n",
      "       0.00310769, 0.02283635, 0.21469507, 0.00295172, 0.02235246,\n",
      "       0.21189423, 0.0028399 , 0.02164588, 0.21308713, 0.00317054,\n",
      "       0.02311635, 0.24243636, 0.0028861 , 0.02212648, 0.21408916,\n",
      "       0.00313439, 0.02171917, 0.21402946, 0.0029923 , 0.02232175,\n",
      "       0.21533303, 0.00311856, 0.02701569, 0.22046127, 0.00313435,\n",
      "       0.02272797, 0.21489835, 0.00282726, 0.02282887, 0.21629133,\n",
      "       0.00328894, 0.022401  , 0.21578379, 0.00274863, 0.02210274,\n",
      "       0.21432099, 0.00297189, 0.02198548, 0.21509109, 0.00298986,\n",
      "       0.02248235, 0.21956429]), 'std_fit_time': array([3.16643204e-03, 3.77580703e-03, 7.58040577e-03, 3.67621546e-04,\n",
      "       3.15047952e-04, 1.08681411e-02, 5.17914932e-04, 1.47007790e-04,\n",
      "       1.22796668e-03, 4.96551705e-04, 4.04587902e-04, 1.13627324e-03,\n",
      "       2.77974977e-04, 4.14839712e-04, 4.28877599e-04, 5.22693324e-04,\n",
      "       2.56331327e-04, 1.14522705e-03, 3.13581602e-04, 2.47804908e-04,\n",
      "       1.14173021e-03, 1.37132149e-04, 7.85893653e-04, 7.10569294e-04,\n",
      "       4.46669083e-04, 2.34019659e-04, 7.20793269e-04, 4.45803947e-04,\n",
      "       1.11220030e-03, 2.19074155e-03, 5.21196856e-04, 1.74089413e-04,\n",
      "       1.41698847e-03, 4.18275690e-04, 1.27001791e-03, 5.65140012e-03,\n",
      "       4.46000775e-04, 4.03245553e-03, 2.13037964e-03, 8.56805779e-04,\n",
      "       8.07745325e-03, 1.09499665e-02, 1.71852072e-04, 4.77267157e-04,\n",
      "       1.06936418e-02, 2.87888024e-04, 6.52472810e-04, 1.13304893e-03,\n",
      "       2.69785699e-04, 9.18799387e-04, 8.54172988e-03, 3.68073608e-04,\n",
      "       7.66681495e-05, 7.56103774e-04, 4.01778406e-04, 4.53233039e-04,\n",
      "       1.01277214e-03, 5.51643377e-04, 9.56750170e-04, 6.65836111e-04,\n",
      "       4.41470880e-04, 6.37499014e-04, 1.27577338e-03, 3.73226478e-04,\n",
      "       3.59291106e-04, 4.07316092e-04, 3.57979081e-04, 1.24877604e-03,\n",
      "       2.01925674e-03, 7.46710851e-05, 4.60803400e-04, 1.93725307e-03,\n",
      "       4.12945676e-04, 3.03809774e-04, 1.10684849e-02, 4.66307432e-04,\n",
      "       1.76125405e-03, 8.94806705e-03, 4.32866600e-04, 3.63742011e-04,\n",
      "       3.30036639e-03, 7.90214389e-04, 3.89028214e-04, 2.63176325e-03,\n",
      "       4.25359727e-04, 2.17055881e-03, 1.35869353e-02, 1.43302490e-04,\n",
      "       5.54452965e-04, 2.64516439e-03, 5.17271481e-04, 5.40128396e-04,\n",
      "       5.46370128e-04, 8.07443723e-05, 5.02286909e-04, 1.03255654e-03,\n",
      "       8.25237756e-05, 4.68282024e-04, 2.60919584e-03, 2.02420974e-04,\n",
      "       4.06047730e-04, 6.29385956e-03, 5.73726208e-04, 4.83900579e-04,\n",
      "       3.46261258e-04, 4.20350593e-04, 3.29896630e-04, 1.24831347e-02,\n",
      "       4.65333253e-04, 1.22904445e-03, 1.14456140e-02, 5.47345690e-04,\n",
      "       1.02346466e-03, 9.66297738e-03, 1.73210925e-04, 6.36610752e-04,\n",
      "       8.68058561e-03, 1.80587933e-04, 5.29178694e-04, 2.91774435e-03,\n",
      "       7.79046562e-05, 1.42083633e-03, 2.26538787e-03, 3.08566126e-04,\n",
      "       3.80631975e-04, 1.75302957e-03, 8.42547646e-05, 9.09251531e-04,\n",
      "       5.01704575e-03, 3.43198012e-04, 3.93580229e-04, 1.26127896e-03,\n",
      "       4.98895680e-04, 4.51368085e-04, 1.16131206e-03, 5.79677521e-04,\n",
      "       4.58581155e-04, 8.93483585e-04, 6.25253738e-04, 5.46270455e-04,\n",
      "       1.44571229e-03, 4.26337007e-04, 1.05984518e-03, 5.26138205e-04,\n",
      "       4.45510991e-04, 1.23415498e-04, 1.69152227e-03, 1.22328519e-04,\n",
      "       3.93518092e-04, 3.61549876e-03, 3.72425432e-04, 1.05448791e-03,\n",
      "       1.30116120e-03, 3.67713196e-04, 3.79396737e-04, 2.71889924e-02,\n",
      "       3.80680657e-04, 9.69859039e-04, 3.78637037e-04, 3.84052101e-04,\n",
      "       1.45956996e-04, 1.48172944e-02, 3.56897072e-04, 6.47531967e-04,\n",
      "       4.18925471e-03, 1.23856063e-04, 5.94820108e-04, 1.00970696e-03,\n",
      "       3.10148025e-05, 4.80702200e-04, 2.95309238e-03, 5.18287545e-04,\n",
      "       1.80534910e-03, 2.27375795e-03, 2.78834580e-04, 1.45875798e-03,\n",
      "       1.61887650e-03, 6.36903869e-04, 1.52603715e-04, 1.52692169e-03,\n",
      "       4.36487509e-04, 5.91396224e-04, 2.20474744e-03, 4.21570812e-04,\n",
      "       1.58119093e-03, 1.61948352e-03, 2.94118617e-04, 1.18996110e-03,\n",
      "       2.57409276e-03, 1.67546939e-04, 5.60682771e-04, 7.20831967e-04,\n",
      "       5.76526008e-04, 1.31313144e-04, 1.06283478e-03, 2.53729815e-04,\n",
      "       8.18649554e-05, 1.45581973e-03, 4.01492215e-04, 4.06900433e-03,\n",
      "       5.63841571e-03, 2.97369912e-04, 5.22527709e-04, 1.08456952e-03,\n",
      "       3.79830402e-04, 4.02483526e-04, 4.91365049e-04, 9.66160809e-05,\n",
      "       3.76575866e-04, 9.13831369e-04, 7.30316855e-05, 1.03245763e-03,\n",
      "       7.79699144e-04, 5.50613903e-04, 9.03201924e-04, 1.22334360e-03,\n",
      "       1.11388731e-04, 1.69055105e-03, 2.45028578e-03, 2.57565276e-04,\n",
      "       2.67358681e-03, 2.33396379e-03, 3.89733889e-04, 2.81357934e-03,\n",
      "       3.61157978e-03, 4.86443912e-04, 5.02894233e-04, 8.97197891e-04,\n",
      "       9.17214587e-07, 3.63607797e-04, 1.81267651e-03, 3.71701878e-04,\n",
      "       9.03361212e-04, 1.60452222e-02, 4.86907493e-04, 6.94000742e-04,\n",
      "       1.79638524e-03, 3.65193638e-04, 1.05776287e-03, 1.28397556e-03,\n",
      "       3.48935560e-04, 4.18564947e-04, 4.93201178e-03, 3.60246197e-04,\n",
      "       3.91714937e-04, 1.92354420e-03, 1.60222210e-04, 1.55262679e-03,\n",
      "       8.55557284e-04, 3.88437373e-04, 3.37426045e-04, 2.35915048e-03,\n",
      "       3.65431510e-04, 1.34687571e-03, 9.52777725e-03, 2.44590040e-04,\n",
      "       8.25383036e-04, 1.34588594e-02, 4.64845809e-04, 5.64948217e-04,\n",
      "       5.60910718e-03, 1.68375083e-04, 1.12075776e-03, 2.28628731e-03,\n",
      "       7.32972700e-05, 2.27591903e-04, 4.29633630e-02, 6.21982492e-04,\n",
      "       2.77980966e-03, 1.99671375e-02, 3.67548995e-04, 4.80238622e-04,\n",
      "       8.58016854e-03, 1.13900059e-04, 3.88471440e-04, 1.73119622e-02,\n",
      "       1.31245902e-03, 8.10202119e-03, 2.47740458e-02, 7.30314785e-04,\n",
      "       9.71562343e-04, 1.90469676e-02, 3.71500963e-04, 9.58919703e-04,\n",
      "       5.96846552e-03, 8.24857168e-05, 6.42976737e-04, 1.86311108e-02,\n",
      "       3.68145833e-04, 1.22028947e-02, 4.80604611e-02, 1.45451423e-03,\n",
      "       1.06638261e-03, 1.28904066e-02, 5.67517828e-04, 1.38823180e-03,\n",
      "       9.57646756e-03, 1.19615340e-04, 4.79494572e-04, 1.23498596e-03,\n",
      "       3.67989789e-04, 4.32846093e-04, 1.07151695e-03, 3.68962980e-04,\n",
      "       4.17786731e-04, 2.03283231e-03, 1.42835805e-04, 4.24402365e-04,\n",
      "       6.32330830e-04, 3.88260187e-04, 5.72282130e-04, 1.08772972e-03,\n",
      "       3.64057023e-04, 1.11324489e-03, 2.05450774e-03, 4.18342976e-04,\n",
      "       7.96232924e-04, 7.01505270e-04, 3.42260260e-04, 5.60972958e-04,\n",
      "       1.11648824e-03, 5.92880589e-04, 4.97700437e-04, 9.56556238e-04,\n",
      "       3.61807150e-04, 8.74397699e-04, 1.20891001e-01, 1.43258451e-03,\n",
      "       1.97813001e-02, 2.68927399e-02, 6.88108545e-04, 6.61281312e-03,\n",
      "       7.00324465e-03, 4.95468915e-05, 1.00721987e-03, 1.86900156e-02,\n",
      "       1.01963666e-03, 2.87684344e-03, 1.65217759e-02, 3.32079428e-04,\n",
      "       4.61133519e-04, 3.65033841e-03, 9.68122707e-05, 8.88640795e-04,\n",
      "       2.50643327e-03, 1.71181458e-04, 9.52725339e-04, 1.11515337e-03,\n",
      "       2.50579238e-04, 2.36394672e-03, 2.34445200e-03, 4.00083272e-04,\n",
      "       4.14303647e-04, 1.03083136e-03, 3.33129592e-04, 5.19405989e-04,\n",
      "       1.23448958e-02, 1.23627871e-03, 5.11965150e-04, 2.08158902e-03,\n",
      "       2.61137274e-04, 1.21223968e-03, 1.04459675e-02, 9.40461903e-05,\n",
      "       2.94160901e-04, 3.80781928e-03, 4.12434246e-04, 1.47556528e-03,\n",
      "       6.91756923e-03, 1.10539411e-04, 1.61118109e-03, 3.34306345e-03,\n",
      "       3.37707387e-04, 6.14176133e-04, 2.31181653e-03, 9.74549467e-05,\n",
      "       1.43613054e-03, 5.95454854e-03, 3.78390178e-04, 5.38114327e-04,\n",
      "       1.18092113e-03, 6.89058567e-05, 4.05078221e-04, 1.75410513e-03,\n",
      "       3.00124883e-04, 5.68283606e-04, 7.63452006e-04, 1.01756915e-04,\n",
      "       2.12204476e-03, 3.65468604e-03, 1.45123852e-04, 5.70016325e-04,\n",
      "       7.19797601e-04, 2.33962308e-04, 1.03608057e-04, 3.00348363e-03,\n",
      "       3.32760550e-04, 3.41541870e-04, 2.10071318e-03, 1.86690876e-04,\n",
      "       1.95999228e-03, 4.99221628e-02, 4.84658151e-04, 3.62313581e-04,\n",
      "       1.57429929e-03, 1.73499190e-04, 6.62227765e-05, 1.79066253e-03,\n",
      "       3.76541418e-04, 6.92849517e-04, 2.77607100e-03, 1.22272280e-04,\n",
      "       7.13146439e-03, 1.07189943e-02, 5.69953775e-04, 6.96496174e-04,\n",
      "       7.98438043e-04, 4.16369461e-04, 2.92212619e-04, 1.07049755e-03,\n",
      "       3.38082905e-04, 6.00886428e-04, 8.59465445e-04, 3.03522062e-04,\n",
      "       3.17454659e-04, 2.68813771e-03, 5.45084713e-04, 7.09869279e-04,\n",
      "       8.51557081e-04, 6.44410456e-04, 6.89387186e-04, 8.83050027e-03]), 'mean_score_time': array([0.00145779, 0.00522733, 0.04571066, 0.00114231, 0.00542731,\n",
      "       0.04405885, 0.00141859, 0.00529828, 0.04428091, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.0010325 , 0.00532069, 0.0502522 , 0.00143046,\n",
      "       0.00525775, 0.04672399, 0.00134444, 0.00519352, 0.04908895,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.001302  , 0.00513358, 0.04478912,\n",
      "       0.00107708, 0.00597415, 0.04486661, 0.00144215, 0.00526314,\n",
      "       0.04435811, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.00140023, 0.00546141,\n",
      "       0.04672847, 0.00142183, 0.00549641, 0.04545412, 0.00139499,\n",
      "       0.00489287, 0.04552536, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00103893, 0.00514202, 0.04497056, 0.00174017,\n",
      "       0.00519104, 0.05009341, 0.00120001, 0.00506296, 0.04404526,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.00143991, 0.0051064 , 0.04513488,\n",
      "       0.00144782, 0.00524063, 0.0463161 , 0.00124879, 0.00530114,\n",
      "       0.04462948, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.00186114, 0.00972052,\n",
      "       0.05912952, 0.00148258, 0.0055912 , 0.05095224, 0.00140362,\n",
      "       0.0054966 , 0.04807239, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.00121322,\n",
      "       0.00528703, 0.05583391, 0.00198884, 0.00624833, 0.05442991,\n",
      "       0.00121813, 0.00641088, 0.05214877, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        ]), 'std_score_time': array([4.33204146e-04, 1.42717369e-04, 1.10251826e-03, 2.07204944e-04,\n",
      "       9.99484426e-04, 3.09721184e-04, 4.61653805e-04, 3.02925328e-04,\n",
      "       8.64355834e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       6.47785693e-05, 3.26179293e-04, 1.19152459e-02, 4.55746513e-04,\n",
      "       3.84725139e-04, 1.51188048e-03, 4.28041740e-04, 5.98626967e-04,\n",
      "       9.15468495e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       4.01040631e-04, 7.14714163e-05, 4.67981076e-04, 8.82740853e-05,\n",
      "       1.27814907e-03, 5.31877393e-04, 4.11368232e-04, 4.68176327e-04,\n",
      "       7.57209549e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       4.89590332e-04, 5.08010398e-04, 8.69556123e-04, 5.85843122e-04,\n",
      "       4.12862965e-04, 1.34443817e-03, 3.94302179e-04, 3.89186280e-04,\n",
      "       2.41618054e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       8.13150114e-05, 1.77823528e-04, 6.56092294e-04, 3.88162593e-04,\n",
      "       3.65049394e-04, 1.03577145e-02, 3.99804183e-04, 7.63929142e-05,\n",
      "       8.84703833e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       4.62134933e-04, 1.05219756e-04, 5.74631602e-04, 5.58466513e-04,\n",
      "       3.88269393e-04, 1.29767026e-03, 3.85525199e-04, 3.59870005e-04,\n",
      "       1.00769013e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       4.33177031e-04, 2.13077668e-03, 6.23502294e-03, 5.15418743e-04,\n",
      "       5.16548945e-04, 1.06078060e-02, 4.88807313e-04, 5.36352841e-04,\n",
      "       7.39313823e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       3.89351500e-04, 3.95768568e-04, 1.27143528e-02, 6.30936945e-04,\n",
      "       1.28518450e-03, 1.24978057e-02, 3.93219431e-04, 1.29529082e-03,\n",
      "       4.04429048e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]), 'param_class_weight': masked_array(data=['balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample',\n",
      "                   'balanced_subsample', 'balanced_subsample'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
      "                   'entropy', 'entropy', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
      "                   'log_loss', 'log_loss'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
      "                   'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
      "                   'log2', 'log2', 'log2', 'log2', 'log2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_samples_leaf': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 10.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 10.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 100.0, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 10.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0, 100.0, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
      "                   1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
      "                   10.0, 10.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
      "                   100.0, 100.0, 100.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_samples_split': masked_array(data=[2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100,\n",
      "                   2, 2, 2, 20, 20, 20, 100, 100, 100, 2, 2, 2, 20, 20,\n",
      "                   20, 100, 100, 100, 2, 2, 2, 20, 20, 20, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
      "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
      "                   100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000,\n",
      "                   10, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 1.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 10.0, 'min_samples_split': 100, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 2, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 20, 'n_estimators': 1000}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 10}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 100}, {'class_weight': 'balanced_subsample', 'criterion': 'log_loss', 'max_features': 'log2', 'min_samples_leaf': 100.0, 'min_samples_split': 100, 'n_estimators': 1000}], 'split0_test_score': array([0.94470736, 0.96328363, 0.92656726, 0.96296296, 0.94470736,\n",
      "       0.92656726, 0.96328363, 0.94510038, 0.92697811,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.90850063, 0.96328363, 0.96328363, 0.98156912,\n",
      "       0.96328363, 0.94470736, 0.94510038, 0.92656726, 0.94510038,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.90850063, 0.94470736, 0.92656726,\n",
      "       0.96328363, 0.90850063, 0.92656726, 0.94510038, 0.94510038,\n",
      "       0.96328363,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.92697811, 0.94470736,\n",
      "       0.98156912, 0.92697811, 0.92656726, 0.94510038, 0.90887713,\n",
      "       0.96328363, 0.94510038,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.92697811, 0.96296296, 0.92656726, 0.96328363,\n",
      "       0.94470736, 0.96328363, 0.94510038, 0.94510038, 0.94510038,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.92656726, 0.94510038, 0.94510038,\n",
      "       0.94470736, 0.94510038, 0.96328363, 0.94470736, 0.92697811,\n",
      "       0.94510038,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.98156912, 0.94510038,\n",
      "       0.96328363, 0.92697811, 0.96328363, 0.94470736, 0.90850063,\n",
      "       0.96328363, 0.94510038,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.92697811,\n",
      "       0.98156912, 0.96328363, 0.94470736, 0.96328363, 0.96328363,\n",
      "       0.90850063, 0.94510038, 0.94510038,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'split1_test_score': array([0.92351003, 0.96226415, 0.94304148, 0.92351003, 0.9245283 ,\n",
      "       0.94304148, 0.9436995 , 0.94304148, 0.9436995 ,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.9245283 , 0.94304148, 0.94304148, 0.9436995 ,\n",
      "       0.94304148, 0.94304148, 0.94416086, 0.96226415, 0.94304148,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.9245283 , 0.96226415, 0.94304148,\n",
      "       0.9421662 , 0.9436995 , 0.94304148, 0.92527158, 0.9436995 ,\n",
      "       0.9436995 ,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.9436995 , 0.9436995 ,\n",
      "       0.94304148, 0.9436995 , 0.96226415, 0.94304148, 0.81505561,\n",
      "       0.94304148, 0.96226415,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.9245283 , 0.94304148, 0.94304148, 0.92351003,\n",
      "       0.94304148, 0.94304148, 0.94304148, 0.9245283 , 0.9436995 ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.88679245, 0.96226415, 0.94304148,\n",
      "       0.96226415, 0.9245283 , 0.94304148, 0.92527158, 0.94304148,\n",
      "       0.9436995 ,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.94304148, 0.9245283 ,\n",
      "       0.94304148, 0.92351003, 0.94304148, 0.94304148, 0.9436995 ,\n",
      "       0.9436995 , 0.98123317,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.81505561,\n",
      "       0.94304148, 0.94304148, 0.96175502, 0.96226415, 0.94304148,\n",
      "       0.96226415, 0.9436995 , 0.96226415,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'split2_test_score': array([0.94402307, 0.90703509, 0.90703509, 0.86992495, 0.88851518,\n",
      "       0.90703509, 0.88851518, 0.90703509, 0.90703509,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.90703509, 0.90703509, 0.92552426, 0.92552426,\n",
      "       0.92552426, 0.92552426, 0.88851518, 0.92552426, 0.90703509,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.88851518, 0.90703509, 0.90703509,\n",
      "       0.90703509, 0.90703509, 0.90703509, 0.92552426, 0.92552426,\n",
      "       0.90703509,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.92552426, 0.90703509,\n",
      "       0.92552426, 0.86992495, 0.90703509, 0.90703509, 0.94402307,\n",
      "       0.90703509, 0.90703509,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.962573  , 0.90703509, 0.90703509, 0.92552426,\n",
      "       0.92552426, 0.90703509, 0.92552426, 0.92552426, 0.90703509,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.86992495, 0.90703509, 0.92552426,\n",
      "       0.90703509, 0.88851518, 0.90703509, 0.90703509, 0.90703509,\n",
      "       0.90703509,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.88851518, 0.92552426,\n",
      "       0.90703509, 0.962573  , 0.92552426, 0.90703509, 0.85122533,\n",
      "       0.88851518, 0.90703509,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.92552426,\n",
      "       0.90703509, 0.92552426, 0.86992495, 0.92552426, 0.90703509,\n",
      "       0.88851518, 0.90703509, 0.90703509,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'split3_test_score': array([0.98103098, 0.98103098, 0.98103098, 0.96226415, 0.98103098,\n",
      "       0.96226415, 0.96226415, 0.96226415, 0.96226415,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.96226415, 0.96226415, 0.96226415, 0.96226415,\n",
      "       0.96226415, 0.96226415, 0.98103098, 0.96226415, 0.96226415,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.9436511 , 0.96226415, 0.96226415,\n",
      "       0.98103098, 0.98103098, 0.96226415, 0.90388666, 0.96226415,\n",
      "       0.96226415,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.96226415, 0.96226415,\n",
      "       0.96226415, 0.96226415, 0.96226415, 0.96226415, 0.90388666,\n",
      "       0.96226415, 0.96226415,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.96226415, 0.96226415, 0.96226415, 0.96226415,\n",
      "       0.96226415, 0.96226415, 0.96226415, 0.96226415, 0.96226415,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.96182604, 0.96226415, 0.96226415,\n",
      "       0.9436511 , 0.96226415, 0.96226415, 0.9436511 , 0.96226415,\n",
      "       0.96226415,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.96226415, 0.96226415,\n",
      "       0.96226415, 0.96226415, 0.96226415, 0.96226415, 0.98121703,\n",
      "       0.96226415, 0.96226415,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.96226415,\n",
      "       0.98103098, 0.96226415, 0.96226415, 0.96226415, 0.96226415,\n",
      "       0.96226415, 0.96226415, 0.96226415,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'split4_test_score': array([0.96182604, 1.        , 1.        , 0.98103098, 1.        ,\n",
      "       1.        , 0.98103098, 0.98121703, 1.        ,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.98103098, 0.98103098, 1.        , 0.96226415,\n",
      "       1.        , 1.        , 0.96226415, 1.        , 1.        ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.98103098, 1.        , 1.        ,\n",
      "       0.98103098, 1.        , 1.        , 0.96182604, 1.        ,\n",
      "       1.        ,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 1.        , 0.98103098,\n",
      "       1.        , 0.96182604, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        ,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.98121703, 1.        , 1.        , 1.        ,\n",
      "       0.98103098, 1.        , 0.96182604, 1.        , 1.        ,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.98103098, 1.        , 1.        ,\n",
      "       0.96182604, 1.        , 1.        , 1.        , 0.98103098,\n",
      "       1.        ,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.96182604, 1.        , 0.96226415,\n",
      "       1.        , 1.        ,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 1.        ,\n",
      "       1.        , 1.        , 0.98103098, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'mean_test_score': array([0.9510195 , 0.96272277, 0.95153496, 0.93993862, 0.94775636,\n",
      "       0.9477816 , 0.94775869, 0.94773163, 0.94799537,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.93667183, 0.95133107, 0.95882271, 0.95506424,\n",
      "       0.95882271, 0.95510745, 0.94421431, 0.95532396, 0.95148822,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.92924524, 0.95525415, 0.9477816 ,\n",
      "       0.95490938, 0.94805324, 0.9477816 , 0.93232178, 0.95531766,\n",
      "       0.95525647,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.95169321, 0.94774742,\n",
      "       0.9624798 , 0.93293855, 0.95162613, 0.95148822, 0.91436849,\n",
      "       0.95512487, 0.95533275,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.95151212, 0.95506074, 0.9477816 , 0.95491642,\n",
      "       0.95131365, 0.95512487, 0.94755126, 0.95148342, 0.95161982,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.92522834, 0.95533275, 0.95518605,\n",
      "       0.94389675, 0.9440816 , 0.95512487, 0.94413303, 0.94406996,\n",
      "       0.95161982,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.95507799, 0.95148342,\n",
      "       0.95512487, 0.95506506, 0.95118791, 0.95140962, 0.92938133,\n",
      "       0.95155249, 0.95912656,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.92596443,\n",
      "       0.96253534, 0.95882271, 0.94393649, 0.96266724, 0.95512487,\n",
      "       0.94430882, 0.95161982, 0.95533275,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'std_test_score': array([0.01930303, 0.03106904, 0.03433552, 0.03972139, 0.03973652,\n",
      "       0.03184117, 0.03188965, 0.02456663, 0.03176792,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.02980644, 0.02520052, 0.02484007, 0.01901554,\n",
      "       0.02484007, 0.02528032, 0.03093362, 0.02759439, 0.03019187,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.03162537, 0.03012236, 0.03184117,\n",
      "       0.02788957, 0.03755489, 0.03184117, 0.01968889, 0.02518645,\n",
      "       0.03024696,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.02758565, 0.02450164,\n",
      "       0.02651707, 0.03410723, 0.03219526, 0.03019187, 0.06035764,\n",
      "       0.03029834, 0.03009524,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan, 0.02213764, 0.03028134, 0.03184117, 0.02830826,\n",
      "       0.01886901, 0.03029834, 0.01365117, 0.02796885, 0.03015618,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan, 0.04240496, 0.03009524, 0.02524846,\n",
      "       0.02009139, 0.03722491, 0.03029834, 0.03115667, 0.02593035,\n",
      "       0.03015618,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan, 0.03833248, 0.02796885,\n",
      "       0.03029834, 0.0279712 , 0.0148805 , 0.03020891, 0.04586413,\n",
      "       0.03642772, 0.031873  ,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan, 0.06180764,\n",
      "       0.03337143, 0.02484007, 0.03874973, 0.02355437, 0.03029834,\n",
      "       0.04035208, 0.03015618, 0.03009524,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan,        nan,        nan,        nan,\n",
      "              nan,        nan]), 'rank_test_score': array([ 45,   1,  35,  64,  53,  48,  52,  55,  47, 307, 306, 305, 304,\n",
      "       303, 302, 301, 300, 299, 308, 298, 296, 295, 294, 293, 292, 291,\n",
      "       290, 289, 288, 297, 309, 310, 311, 332, 331, 330,  65,  42,   6,\n",
      "        25,   6,  22,  58,  12,  37, 329, 328, 327, 326, 325, 324, 323,\n",
      "       322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 287, 286,\n",
      "       285, 261, 259, 258, 257, 256, 255,  69,  15,  48,  28,  46,  48,\n",
      "        67,  13,  14, 254, 253, 252, 251, 260, 250, 248, 247, 246, 245,\n",
      "       244, 243, 242, 241, 240, 249, 262, 273, 263, 283, 282, 281, 280,\n",
      "       279, 278, 277, 276,  29,  54,   4,  66,  30,  37,  72,  17,   9,\n",
      "       275, 284, 274, 272, 271, 270, 269, 268, 267, 266, 265, 264, 333,\n",
      "       334, 335, 336, 405, 404, 403, 402, 401, 400, 399, 398, 397, 406,\n",
      "       396, 394, 393, 392, 391, 390, 389, 388, 387, 386, 395, 407, 408,\n",
      "       409, 430, 429, 428, 427, 426, 425, 424, 423, 422, 421, 420, 419,\n",
      "       418, 417, 416, 415, 414, 413, 412, 411, 410, 385, 431, 384, 382,\n",
      "       356, 355, 354, 353, 352, 351, 350, 349, 348, 357, 347, 345, 344,\n",
      "       343, 342, 341, 340, 339, 338, 337, 346, 358, 359, 360, 381, 380,\n",
      "       379, 378, 377, 376, 375, 374, 373, 238,  36,  26,  48,  27,  43,\n",
      "        17,  56,  39,  31, 371, 370, 369, 368, 367, 366, 365, 364, 363,\n",
      "       362, 361, 239, 383, 237, 198,  82,  83,  97,  76, 124,  99, 125,\n",
      "       126, 127, 129, 132, 133,  71,   9,  16,  63,  60,  17,  59,  61,\n",
      "        31, 116, 115, 114, 113, 112, 111, 110, 117, 109, 107, 106, 105,\n",
      "       104, 103, 102, 101, 108, 119, 128, 120, 138, 136, 135, 134, 131,\n",
      "       130, 137,  23,  39,  17,  24,  44,  41,  68,  34,   5, 123, 122,\n",
      "       121, 100, 118,  98,  84,  79,  78,  77,  75,  74,  81,  73,  80,\n",
      "        85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  70,\n",
      "         3,   6,  62,   2,  17,  57,  31,   9, 212, 140, 210, 209, 208,\n",
      "       207, 206, 205, 204, 203, 202, 201, 200, 199, 139, 197, 196, 195,\n",
      "       194, 193, 192, 191, 190, 211, 213, 236, 214, 235, 234, 233, 232,\n",
      "       231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220, 219,\n",
      "       218, 217, 216, 215, 189, 188, 187, 186, 160, 159, 158, 157, 156,\n",
      "       155, 154, 153, 152, 161, 151, 149, 148, 147, 146, 145, 144, 143,\n",
      "       142, 141, 150, 162, 163, 164, 185, 184, 183, 182, 181, 180, 179,\n",
      "       178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166,\n",
      "       165, 372, 432])}\n"
     ]
    }
   ],
   "source": [
    "re=grid.cv_results_\n",
    "print(re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  1],\n",
       "       [ 6, 76]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_predictions = grid.predict(x_test) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, grid_predictions)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93        51\n",
      "           1       0.99      0.93      0.96        82\n",
      "\n",
      "    accuracy                           0.95       133\n",
      "   macro avg       0.94      0.95      0.95       133\n",
      "weighted avg       0.95      0.95      0.95       133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print classification report \n",
    "from sklearn.metrics import classification_report\n",
    "clf_report = classification_report(y_test, grid_predictions)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_macro value for best parameter {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}: 0.9477705902916346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_macro=f1_score(y_test,grid_predictions,average='weighted')\n",
    "print(\"The f1_macro value for best parameter {}:\".format(grid.best_params_),f1_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966523194643712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test,grid.predict_proba(x_test)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_macro value for best parameter {'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 100}: 0.9477705902916346\n",
      "[[50  1]\n",
      " [ 6 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93        51\n",
      "           1       0.99      0.93      0.96        82\n",
      "\n",
      "    accuracy                           0.95       133\n",
      "   macro avg       0.94      0.95      0.95       133\n",
      "weighted avg       0.95      0.95      0.95       133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The f1_macro value for best parameter {}:\".format(grid.best_params_),f1_macro)\n",
    "print((cm))\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame.from_dict(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 19)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gin...</td>\n",
       "      <td>0.944707</td>\n",
       "      <td>0.923510</td>\n",
       "      <td>0.944023</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>0.961826</td>\n",
       "      <td>0.951019</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.046170</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gin...</td>\n",
       "      <td>0.963284</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.907035</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962723</td>\n",
       "      <td>0.031069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.427404</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.045711</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gin...</td>\n",
       "      <td>0.926567</td>\n",
       "      <td>0.943041</td>\n",
       "      <td>0.907035</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951535</td>\n",
       "      <td>0.034336</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gin...</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.923510</td>\n",
       "      <td>0.869925</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>0.939939</td>\n",
       "      <td>0.039721</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043285</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gin...</td>\n",
       "      <td>0.944707</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.888515</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947756</td>\n",
       "      <td>0.039737</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': 'balanced_subsample', 'criter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.215091</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'class_weight': 'balanced_subsample', 'criter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': 'balanced_subsample', 'criter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.022482</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': 'balanced_subsample', 'criter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.219564</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'class_weight': 'balanced_subsample', 'criter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.007251      0.003166         0.001458        0.000433   \n",
       "1         0.046170      0.003776         0.005227        0.000143   \n",
       "2         0.427404      0.007580         0.045711        0.001103   \n",
       "3         0.005329      0.000368         0.001142        0.000207   \n",
       "4         0.043285      0.000315         0.005427        0.000999   \n",
       "..             ...           ...              ...             ...   \n",
       "427       0.021985      0.000710         0.000000        0.000000   \n",
       "428       0.215091      0.000852         0.000000        0.000000   \n",
       "429       0.002990      0.000644         0.000000        0.000000   \n",
       "430       0.022482      0.000689         0.000000        0.000000   \n",
       "431       0.219564      0.008831         0.000000        0.000000   \n",
       "\n",
       "     param_class_weight param_criterion param_max_features  \\\n",
       "0              balanced            gini               sqrt   \n",
       "1              balanced            gini               sqrt   \n",
       "2              balanced            gini               sqrt   \n",
       "3              balanced            gini               sqrt   \n",
       "4              balanced            gini               sqrt   \n",
       "..                  ...             ...                ...   \n",
       "427  balanced_subsample        log_loss               log2   \n",
       "428  balanced_subsample        log_loss               log2   \n",
       "429  balanced_subsample        log_loss               log2   \n",
       "430  balanced_subsample        log_loss               log2   \n",
       "431  balanced_subsample        log_loss               log2   \n",
       "\n",
       "    param_min_samples_leaf param_min_samples_split param_n_estimators  \\\n",
       "0                      0.1                       2                 10   \n",
       "1                      0.1                       2                100   \n",
       "2                      0.1                       2               1000   \n",
       "3                      0.1                      20                 10   \n",
       "4                      0.1                      20                100   \n",
       "..                     ...                     ...                ...   \n",
       "427                    100                      20                100   \n",
       "428                    100                      20               1000   \n",
       "429                    100                     100                 10   \n",
       "430                    100                     100                100   \n",
       "431                    100                     100               1000   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "0    {'class_weight': 'balanced', 'criterion': 'gin...           0.944707   \n",
       "1    {'class_weight': 'balanced', 'criterion': 'gin...           0.963284   \n",
       "2    {'class_weight': 'balanced', 'criterion': 'gin...           0.926567   \n",
       "3    {'class_weight': 'balanced', 'criterion': 'gin...           0.962963   \n",
       "4    {'class_weight': 'balanced', 'criterion': 'gin...           0.944707   \n",
       "..                                                 ...                ...   \n",
       "427  {'class_weight': 'balanced_subsample', 'criter...                NaN   \n",
       "428  {'class_weight': 'balanced_subsample', 'criter...                NaN   \n",
       "429  {'class_weight': 'balanced_subsample', 'criter...                NaN   \n",
       "430  {'class_weight': 'balanced_subsample', 'criter...                NaN   \n",
       "431  {'class_weight': 'balanced_subsample', 'criter...                NaN   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0             0.923510           0.944023           0.981031   \n",
       "1             0.962264           0.907035           0.981031   \n",
       "2             0.943041           0.907035           0.981031   \n",
       "3             0.923510           0.869925           0.962264   \n",
       "4             0.924528           0.888515           0.981031   \n",
       "..                 ...                ...                ...   \n",
       "427                NaN                NaN                NaN   \n",
       "428                NaN                NaN                NaN   \n",
       "429                NaN                NaN                NaN   \n",
       "430                NaN                NaN                NaN   \n",
       "431                NaN                NaN                NaN   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.961826         0.951019        0.019303               45  \n",
       "1             1.000000         0.962723        0.031069                1  \n",
       "2             1.000000         0.951535        0.034336               35  \n",
       "3             0.981031         0.939939        0.039721               64  \n",
       "4             1.000000         0.947756        0.039737               53  \n",
       "..                 ...              ...             ...              ...  \n",
       "427                NaN              NaN             NaN              167  \n",
       "428                NaN              NaN             NaN              166  \n",
       "429                NaN              NaN             NaN              165  \n",
       "430                NaN              NaN             NaN              372  \n",
       "431                NaN              NaN             NaN              432  \n",
       "\n",
       "[432 rows x 19 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
